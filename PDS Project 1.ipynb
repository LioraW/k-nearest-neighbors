{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef8201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import starements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2c520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(data, training, test):\n",
    "    print(\"Total Data:\")\n",
    "    print(data)\n",
    "    print (\"\\nTraining data:\")\n",
    "    print(training)\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(test)\n",
    "\n",
    "    #https://stackoverflow.com/questions/10374930/matplotlib-annotating-a-3d-scatter-plot\n",
    "    #ax = plt.axes(projection = \"3d\")\n",
    "    #fig = plt.figure(figsize = (10,10))\n",
    "    x1 = data.T[0]\n",
    "    x2 = data.T[1]\n",
    "    labels = data.T[2]\n",
    "\n",
    "    # Create point (1,3,4) on figure \n",
    "    plt.scatter(x1,x2, c=labels, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefc2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(TOTAL_SAMPLE_SIZE,n,d,m,s):\n",
    "\n",
    "    label_names = [\"good\", \"bad\", \"ugly\"]\n",
    "    \n",
    "    #generating the total sample set\n",
    "    rng = np.random.default_rng(seed = 42)\n",
    "    total_sample_set = rng.normal(m, s, d*TOTAL_SAMPLE_SIZE).reshape(TOTAL_SAMPLE_SIZE, d)\n",
    "    \n",
    "    labels = np.random.randint(0, len(label_names), TOTAL_SAMPLE_SIZE)\n",
    "    \n",
    "    # appends the lables to the end of each of the data set... not really nesscary\n",
    "    complete_data = np.concatenate((total_sample_set, labels.T.reshape(labels.shape[0], 1)), axis=1)\n",
    "    \n",
    "    return complete_data\n",
    "\n",
    "def separate_test_and_train_data(data, n):\n",
    "    training_ind = np.random.choice(np.arange(len(data)), n, replace=False) #get random indicies for training data\n",
    "    test_ind = np.setdiff1d(np.arange(len(data)), training_ind) #get the indicies not chosen\n",
    "    \n",
    "    training = data[training_ind]\n",
    "    test = data[test_ind]\n",
    "    \n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8036603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(distance):\n",
    "    return 1/(disance + 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72e2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances(train, point):\n",
    "    return np.linalg.norm((train - point), axis=1)\n",
    "\n",
    "def kneighbors(train, test, k):\n",
    "    \n",
    "    for point in test:\n",
    "        dists = distances(train, point)\n",
    "        print(dists)\n",
    "    #1.  Find its Euclidean distance from each of the n points in the training data set \n",
    "    ##2.  Pick the nearest K points \n",
    "    # 3.  Output the class by weighted voting using the K nearest neighbors in the above step \n",
    "    # (use the inverse of the distance as the weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260cbc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.81182645 4.01151773 4.48338517 2.6512755  2.84013658 7.9510163\n",
      " 4.1361035 ]\n",
      "[3.72630481 3.6452833  4.39722314 3.12226779 2.79568526 7.38170548\n",
      " 4.22664153]\n",
      "[2.72491785 2.67449131 2.97740631 2.21395023 3.5175691  6.05688129\n",
      " 4.21102841]\n"
     ]
    }
   ],
   "source": [
    "'''Number of samples (data points or vectors) in the training set: n \n",
    "• Total number of samples (data points or vectors) in the training set plus test set: \n",
    "TOTAL_SAMPLE_SIZE \n",
    "• Number of features: d \n",
    "• Stipulated number of nearest neighbors: K \n",
    "• Mean and standard deviation of the normal distribution: m and s, respectively \n",
    "• Data structure(s) (typically ndarray) holding all the training data and the corresponding labels: \n",
    "X_train, y_train   \n",
    "• Data structure(s) (typically ndarray) holding all the test data and the corresponding labels: \n",
    "X_test, y_test '''\n",
    "\n",
    "TOTAL_SAMPLE_SIZE = 10 # Total number of samples (data points or vectors) in the training set plus test set\n",
    "n = 7 #Number of samples (data points or vectors) in the training set\n",
    "d = 3 #Number of features\n",
    "K = 3 #Stipulated number of nearest neighbors\n",
    "m = 5 #mean of normal distribution\n",
    "s = 2 #std of normal distrubution\n",
    "\n",
    "data = generate_data(TOTAL_SAMPLE_SIZE, n, d, m, s)\n",
    "\n",
    "training, test = separate_test_and_train_data(data, n)\n",
    "\n",
    "kneighbors(training, test, K)\n",
    "\n",
    "#print_data(data, training, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6873df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
